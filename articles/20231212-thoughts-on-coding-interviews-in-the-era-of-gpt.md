Thoughts on Coding Interviews in the Era of GPT
thoughts-on-coding-interviews-in-the-era-of-gpt
programming,ai

# Thoughts on Coding Interviews in the Era of GPT

GPT seems to have ruined a lot of coding interviews, or at least provides the threat of it.  A typical coding interview go through the process of:

1. Define a problem to be solved by code
2. The interviewee codes the problem in _some lang_
3. The interviewer provides some test inputs, sometimes containing edge cases depending on the engineering level

With GPT comes the threat of #2 no longer coming from the interviewee exclusively.  The pitfalls of eliminating something entirely are well documented but if you need a refresher, see:

- The Prohibition
- War on Drugs
- Banning calculators from math classrooms

A couple of reasonable options for interviewing in the era of GPT that we'll discuss are:

1. Working with GPT
2. Edge casing GPT

# Working with GPT

An interesting option is generating prompts during the interview and using that as an interview starter.  The prompt:

```
i want you to act a technical programming interviewing assistant, we're going to be interviewing a senior engineer in python i'd like you to generate 50 lines of code with two difficult to spot bugs and provide a solution to those bugs
```

The result:

```python
def calculate_average(numbers):
    total = 0
    count = 0
    for num in range(numbers):
        total += num
        count += 1
    average = total / count
    return average

def main():
    data = [10, 20, 30, 40, 50]
    result = calculate_average(data)
    print(f"The average is: {result}")

if __name__ == "__main__":
    main()
```

Okay, so it didn't generate 50 lines of code and the bugs are fairly obvious and not difficult to spot.  However, this is pretty promising.  By tweaking the prompt you can definitely get something usable from this and the interviewee,
given a short period, should be able to walk through this code and find the bugs upon reading without them being able to type all of that into GPT and have it give them the answer.  Finding bugs is what most of us do day to day anyway
so interviewing this way is more representative than `create X from scratch`.

This also presents the opportunity for the interviewee to optimize code they didn't write and gives the interviewer even more insight as to how the interviewee reasons about, and looks at problems they didn't present.

# Edge Casing GPT

This is an unsustainable course but is interesting for higher level engineers and certainly viable for positions where heavy algorithms knowledge is required. This section is mainly talking as the code this would generate
would necessarily be complex.  Asking GPT to generate something such as a GZIP compression/decompression utility is something a lot of programmers are only familiar with in the form of using GZIP and not actually implementing it
so the edge case here is that most people are unfamiliar with how it works and GPT does not have a lot of code to pull from to generate something useful.

Most implementations generated by GPT contain very subtle bugs that cause the files generated to fail when using the `tar xf` or `gunzip` utilities that come on most systems while basic compressions work.  This makes the bugs subtle and
GPT is mainly unhelpful in resolving those bugs so it requires the interviewee to actually think about, debug, and resolve the bugs on their own.

Extrapolating on this knowledge allows you to find edge cases in what GPT has actually been exposed to rather than common programming algorithms people are expected to know and testing for those.  Again, this only works with higher
level positions and probably only useful for pure programming positions where algorithm knowledge is explicitly needed.
